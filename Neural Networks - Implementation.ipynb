{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d22f91",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "A family of algorithms known as neural networks has become increasingly popular during the past few years under\n",
    "the name “deep learning.” While deep learning shows great promise in many machine\n",
    "learning applications, deep learning algorithms are often tailored very carefully to a\n",
    "specific use case. Here, we will only discuss some relatively simple methods, namely\n",
    "multilayer perceptrons for classification and regression, that can serve as a starting\n",
    "point for more involved deep learning methods. Multilayer perceptrons (MLPs) are\n",
    "also known as (vanilla) feed-forward neural networks, or sometimes just neural\n",
    "networks.\n",
    "\n",
    "MLPs can be viewed as generalizations of linear models that perform multiple stages\n",
    "of processing to come to a decision. Remember that the prediction by a linear regressor is given as:\n",
    "\n",
    "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
    "\n",
    "In plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\n",
    "the learned coefficients w[0] to w[p]. We could visualize this graphically as shown in the following figure:\n",
    "\n",
    "<img src=\"perceptron.png\">\n",
    "\n",
    "In an MLP this process of computing weighted sums is repeated multiple times, first\n",
    "computing hidden units that represent an intermediate processing step, which are\n",
    "again combined using weighted sums to yield the final result. Note that there can be several layers of hidden units, each of which will create a more complex representation of the input.\n",
    "\n",
    "<img src=\"MLP.png\">\n",
    "\n",
    "Computing a series of weighted sums is mathematically the same as computing just\n",
    "one weighted sum, so to make this model truly more powerful than a linear model,\n",
    "we need one extra trick. After computing a weighted sum for each hidden unit, a\n",
    "nonlinear function is applied to the result.\n",
    "\n",
    "Let’s look into the workings of the MLP by applying the `MLPClassifier` to the\n",
    "two_moons dataset. Import the `make_moons` method from sklearn library and generate a dataset with 100 samples `noise=0.25` and `random_state=3`. Then make a scatter plot of the data with matplotlib `scatter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6071ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2b66950",
   "metadata": {},
   "source": [
    "Now import the `Perceptron` classifier from sklearn and apply it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09083796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f38cd9ba",
   "metadata": {},
   "source": [
    "Build a function to visualize the input data and the decision function of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, ax=None, cmap='RdBu', figsize=(7,7)):\n",
    "    import numpy as np\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.figure(figsize=figsize)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap,\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73f3b7",
   "metadata": {},
   "source": [
    "Visualize the classifier. Is this shape expected? Why is the perceptron a linear classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd55bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7987b6",
   "metadata": {},
   "source": [
    "Now import the `MLPClassifier`, set the random state to 0 and the solver to lbfgs. Leave the rest of the parameters as default. Fit the MLP model and then plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9c1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57c249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
